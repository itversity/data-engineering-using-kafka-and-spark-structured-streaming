{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797f3477",
   "metadata": {},
   "source": [
    "## Write Data to HDFS using Spark\n",
    "\n",
    "Let us now write the data to HDFS using Spark Structured Streaming APIs. It will take care of writing streaming data frame to HDFS.\n",
    "\n",
    "Here are the steps that are involved.\n",
    "* Create spark session object.\n",
    "* Create streaming data frame by subscribing to Kafka topic leveraging `readStream`.\n",
    "* Apply required transformations to add new fields such as year, month and dayofmonth.\n",
    "* Write the data from streaming data frame to HDFS using appropriate format. We will be using `csv` for now. We can write data to all standard formats using streaming data frame.\n",
    "* We need to specify `checkpointLocation` while writing data to HDFS. The location should be in HDFS itself.\n",
    "* As part of this lecture we will validate whether the files are being generated or not. We will perform detailed validation as part of next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643361aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1'). \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Kafka and Spark Integration'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facdd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = 'w01.itversity.com:9092,w02.itversity.com:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec31bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark. \\\n",
    "  readStream. \\\n",
    "  format('kafka'). \\\n",
    "  option('kafka.bootstrap.servers', kafka_bootstrap_servers). \\\n",
    "  option('subscribe', f'{username}_retail'). \\\n",
    "  load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298e5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, to_date, split, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c5c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING) AS key\", \"CAST(value AS STRING) AS value\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0064be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/itversity/kafka/retail_logs\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R -skipTrash /user/${USER}/kafka/retail_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07080b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f56bafd2e48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"CAST(value AS STRING)\"). \\\n",
    "    withColumn('log_date', to_date(substring(split('value', ' ')[3], 2, 21), '[dd/MMM/yyyy:HH:mm:ss')). \\\n",
    "    withColumn('year', date_format('log_date', 'yyyy')). \\\n",
    "    withColumn('month', date_format('log_date', 'MM')). \\\n",
    "    withColumn('dayofmonth', date_format('log_date', 'dd')). \\\n",
    "    writeStream. \\\n",
    "    partitionBy('year', 'month', 'dayofmonth'). \\\n",
    "    format('csv'). \\\n",
    "    option(\"checkpointLocation\", f'/user/{username}/kafka/retail_logs/gen_logs/checkpoint'). \\\n",
    "    option('path', f'/user/{username}/kafka/retail_logs/gen_logs/data'). \\\n",
    "    trigger(processingTime='30 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4490b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - itversity itversity          0 2021-09-02 13:20 /user/itversity/kafka/retail_logs/gen_logs/checkpoint\n",
      "drwxr-xr-x   - itversity itversity          0 2021-09-02 13:20 /user/itversity/kafka/retail_logs/gen_logs/data\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/${USER}/kafka/retail_logs/gen_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f2ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - itversity itversity          0 2021-09-02 13:20 /user/itversity/kafka/retail_logs/gen_logs/data/_spark_metadata\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R /user/${USER}/kafka/retail_logs/gen_logs/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d12209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

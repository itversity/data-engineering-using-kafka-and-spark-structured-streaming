{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b84ec8",
   "metadata": {},
   "source": [
    "## Transform Data using Spark APIs\n",
    "\n",
    "Let us transform the data using Spark APIs to add new columns using timestamp from the log messages so that data can be partitioned by year, month and then by day of month while writing to target location.\n",
    "* We will start with creating spark session.\n",
    "* Using the spark session's `readStream` we will subscribe to Kafka Topic to create streaming Data Frame.\n",
    "* We can apply required transformations to add columns such as year, month and dayofmonth using the timestamp that is part of each and every message.\n",
    "* As part of the next lecture we will see how to partition the data using these new columns while writing to target. For now, we will validate whether new columns are added or not using `memory` as the target along with `queryName`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2ced39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1'). \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Kafka and Spark Integration'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6c4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = 'w01.itversity.com:9092,w02.itversity.com:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebed58ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark. \\\n",
    "  readStream. \\\n",
    "  format('kafka'). \\\n",
    "  option('kafka.bootstrap.servers', kafka_bootstrap_servers). \\\n",
    "  option('subscribe', f'{username}_retail'). \\\n",
    "  load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828de45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be772c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, date_format, to_date, split, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a08960f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2ec0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dual = spark.createDataFrame(l, schema='dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7fa200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|to_date('2021-Jan-21', 'yyyy-MMM-dd')|\n",
      "+-------------------------------------+\n",
      "|                           2021-01-21|\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dual.select(to_date(lit('2021-Jan-21'), 'yyyy-MMM-dd')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984fd9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+\n",
      "|to_date('31/Dec/2021:00:37:39', 'dd/MMM/yyyy:HH:mm:ss')|\n",
      "+-------------------------------------------------------+\n",
      "|                                             2021-12-31|\n",
      "+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dual.select(to_date(lit('31/Dec/2021:00:37:39'), 'dd/MMM/yyyy:HH:mm:ss')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc2b91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f1c2373e518>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\"). \\\n",
    "    writeStream. \\\n",
    "    format(\"memory\"). \\\n",
    "    queryName(\"log_messages_raw\"). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM log_messages_raw').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd0f475b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f1c2373ea58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\"). \\\n",
    "    withColumn('log_date', to_date(substring(split('value', ' ')[3], 2, 21), 'dd/MMM/yyyy:HH:mm:ss')). \\\n",
    "    withColumn('year', date_format('log_date', 'yyyy')). \\\n",
    "    withColumn('month', date_format('log_date', 'MM')). \\\n",
    "    withColumn('dayofmonth', date_format('log_date', 'dd')). \\\n",
    "    writeStream. \\\n",
    "    format(\"memory\"). \\\n",
    "    queryName(\"log_messages\"). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM log_messages').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8ba4023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|126     |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(1) FROM log_messages').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883a21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c89647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1f6781d-04aa-4518-8b59-f5a7a20faa94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Incremental Load using Archival Process\n",
    "\n",
    "Let us go through the details about reading and writing data to target location leveraging Spark Structured Streaming APIs while dealing with processed files.\n",
    "* As part of data pipelines, we typically delete the files or archive the files which are already processed.\n",
    "* As part of the archival process, typically we move the files to a different location using some low cost storage.\n",
    "* Spark Structured Streaming APIs support both `delete` as well as `archive` using `cleanSource` option. \n",
    "* By default `cleanSource` is turned off. We can set it to either `delete` or `archive`.\n",
    "* When we use `archive`, we also need to set the archive folder using `sourceArchiveDir`. We need to pass the fully qualified path of the archive folder to `sourceArchiveDir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Incremental Loads using Spark Structurd Streaming'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3db9a4b-db22-406b-8c7c-67e2db02c0e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.streaming.schemaInference', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2413ae82-aa4c-407e-bc69-3c86df09e92c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df = spark. \\\n",
    "    readStream. \\\n",
    "    format('json'). \\\n",
    "    option('maxFilesPerTrigger', 8). \\\n",
    "    option('cleanSource', 'archive'). \\\n",
    "    option('sourceArchiveDir', f'/user/{username}/itv-github/streaming/landing/archive/ghactivity'). \\\n",
    "    load(f'/user/{username}/itv-github/streaming/landing/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a871780-dc75-4be9-a579-1f31654178a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3217137-15a6-4e8c-b5a6-a4a7cc4484ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df = ghactivity_df. \\\n",
    "    withColumn('created_year', year('created_at')). \\\n",
    "    withColumn('created_month', lpad(month('created_at'), 2, '0')). \\\n",
    "    withColumn('created_dayofmonth', lpad(dayofmonth('created_at'), 2, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13e4e243-49bb-41cb-9438-8738ed2779e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df. \\\n",
    "    writeStream. \\\n",
    "    partitionBy('created_year', 'created_month', 'created_dayofmonth'). \\\n",
    "    format('parquet'). \\\n",
    "    option(\"checkpointLocation\", f\"/user/{username}/itv-github/streaming/bronze/checkpoint/ghactivity\"). \\\n",
    "    option(\"path\", f\"/user/{username}/itv-github/streaming/bronze/data/ghactivity\"). \\\n",
    "    trigger(processingTime='120 seconds'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35a8fc0d-cd29-4c3d-aed2-9d4712ba5731",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Validating the checkpoint location. We can see multiple folders. These folders will have all the files that are required for the overhead of the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a09e7932-76d4-4f4b-ad1c-2c0d3f66369b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -R /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check last few ones\n",
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources/0/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources/0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/offsets/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/offsets/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fee6dc7e-3083-4c9b-9096-c717748c8d0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Validating the data location. We should see the files in this location as we are just copying the files in the parquet file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9f366c8-8a6f-4e35-878f-1bd79003a696",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/data/ghactivity/created_year=2021/created_month=01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3470cc47-5ed2-462c-b6ea-90bfaa96ffe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/data/ghactivity/created_year=2021/created_month=01/created_dayofmonth=16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fee6dc7e-3083-4c9b-9096-c717748c8d0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Validating the archive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -R /user/${USER}/itv-github/streaming/landing/archive/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "07 Write using Delta file format using Trigger Once",
   "notebookOrigID": 1228141849143135,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

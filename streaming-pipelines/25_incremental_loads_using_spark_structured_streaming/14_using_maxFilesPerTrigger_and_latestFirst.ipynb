{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f1f6781d-04aa-4518-8b59-f5a7a20faa94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using maxFilerPerTrigger and latestFirst\n",
    "\n",
    "Let us go through the details about reading and writing data to target location leveraging Spark Structured Streaming APIs using `maxFilesPerTrigger`. We will also see how to process the latest files first.\n",
    "* `maxFilesPerTrigger` is primarily used to keep the usage of resources under control.\n",
    "* It is useful for baseline loads as well as sudden spikes in incremental loads.\n",
    "* By default old files based upon the timestamp associated with the files will be read first, however we can change the behavior using `latestFirst`.\n",
    "* We will also validate by running `hdfs dfs -ls` command to see if the parquet files are copied or not.\n",
    "* The files that are available at source at this time will be picked up automatically. However, only latest 8 files will be picked as part of the first iteration.\n",
    "\n",
    "```{note}\n",
    "`maxFilesPerTrigger` is not applicable when we trigger job runs using `trigger(once=True)`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Incremental Loads using Spark Structurd Streaming'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3db9a4b-db22-406b-8c7c-67e2db02c0e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.streaming.schemaInference', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2413ae82-aa4c-407e-bc69-3c86df09e92c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df = spark. \\\n",
    "    readStream. \\\n",
    "    format('json'). \\\n",
    "    option('maxFilesPerTrigger', 8). \\\n",
    "    option('latestFirst', True). \\\n",
    "    option('cleanSource', 'delete'). \\\n",
    "    option('path', f'/user/{username}/itv-github/streaming/landing/ghactivity'). \\\n",
    "    load()\n",
    "\n",
    "# We can also pass path directly to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a871780-dc75-4be9-a579-1f31654178a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3217137-15a6-4e8c-b5a6-a4a7cc4484ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df = ghactivity_df. \\\n",
    "    withColumn('created_year', year('created_at')). \\\n",
    "    withColumn('created_month', lpad(month('created_at'), 2, '0')). \\\n",
    "    withColumn('created_dayofmonth', lpad(dayofmonth('created_at'), 2, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13e4e243-49bb-41cb-9438-8738ed2779e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df. \\\n",
    "    writeStream. \\\n",
    "    partitionBy('created_year', 'created_month', 'created_dayofmonth'). \\\n",
    "    format('parquet'). \\\n",
    "    option(\"checkpointLocation\", f\"/user/{username}/itv-github/streaming/bronze/checkpoint/ghactivity\"). \\\n",
    "    option(\"path\", f\"/user/{username}/itv-github/streaming/bronze/data/ghactivity\"). \\\n",
    "    trigger(processingTime='60 seconds'). \\\n",
    "    start()\n",
    "\n",
    "# If the job run is completed before 60 seconds, it will wait for the next run.\n",
    "# If the job run takes more than 60 seconds to complete, then next run will start immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35a8fc0d-cd29-4c3d-aed2-9d4712ba5731",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Validating the checkpoint location. We can see multiple folders. These folders will have all the files that are required for the overhead of the checkpoint.\n",
    "\n",
    "```{note}\n",
    "You can wait for few minutes before running below cells.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a09e7932-76d4-4f4b-ad1c-2c0d3f66369b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -R /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources/0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources/0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources/0/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources/0/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/sources/0/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/offsets/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/offsets/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/${USER}/itv-github/streaming/bronze/checkpoint/ghactivity/offsets/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fee6dc7e-3083-4c9b-9096-c717748c8d0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Validating the data location. We should see the files in this location as we are just copying the files in the parquet file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9f366c8-8a6f-4e35-878f-1bd79003a696",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/data/ghactivity/created_year=2021/created_month=01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3470cc47-5ed2-462c-b6ea-90bfaa96ffe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/data/ghactivity/created_year=2021/created_month=01/created_dayofmonth=15/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fee6dc7e-3083-4c9b-9096-c717748c8d0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Validating the source location to see if the files are deleted or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -R /user/${USER}/itv-github/streaming/landing/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "07 Write using Delta file format using Trigger Once",
   "notebookOrigID": 1228141849143135,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

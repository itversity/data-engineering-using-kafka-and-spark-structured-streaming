{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3ee45ff-2886-4bfc-91ac-d94fcc99f882",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read JSON Data using Spark Structured Streaming\n",
    "\n",
    "Let us understand how to read files using Spark Structured Streaming.\n",
    "* `spark.readStream` exposes several APIs to read data using different file formats.\n",
    "  * `json`\n",
    "  * `csv`\n",
    "  * `parquet`\n",
    "  * `orc`\n",
    "* You can check by typing `spark.readStream.` and then by hitting tab.\n",
    "* We can also pass file format as argument to `spark.readStream.format`.\n",
    "* Depending upon the file format chosen, we need to apply additional options. For example, if we use `csv`, we might have to specify `header` and also custom separator.\n",
    "* Some options are applicable to all formats. Here are commonly used options for all formats.\n",
    "  * `path`\n",
    "  * `maxFilesPerTrigger`\n",
    "  * `latestFirst`\n",
    "  * `maxFileAge`\n",
    "  * `cleanSource` (`archive`, `delete`, `off`). We need to provide additional option for archive.\n",
    "* Here are the examples to read the files using `json` file format:\n",
    "  * Direct API: `spark.readStream.json(f'/user/{username}/itv-github/streaming/landing/ghactivity')`\n",
    "  * Using format: `spark.readStream.format('json').load(f'/user/{username}/itv-github/streaming/landing/ghactivity')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Incremental Loads using Spark Structurd Streaming'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ffb95f51-9ced-4c99-9ed1-3febf12a0f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* When we read `json` files using `spark.readStream`, by default schema will not be inferred.\n",
    "* The below cell will fail as schema is mandatory for `spark.readStream.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6731b93-10c8-4fee-8576-9caec93880c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.json(f'/user/{username}/itv-github/streaming/landing/ghactivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23c46279-3df8-4111-82df-87a189749260",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* We can set `spark.sql.streaming.schemaInference` to `true` so that the schema can be inferred automatically when we use `spark.readStream.json`.\n",
    "* However, you should use it with caution as the whole data will be read every time to apply the schema.\n",
    "* Let us go ahead and try reading `json` files after enabling the **schema inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4c50436-4e00-4926-bffe-ff911bd82dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.streaming.schemaInference', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -R /user/${USER}/itv-github/streaming/landing/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2413ae82-aa4c-407e-bc69-3c86df09e92c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df = spark. \\\n",
    "    readStream. \\\n",
    "    format('json'). \\\n",
    "    load(f'/user/{username}/itv-github/streaming/landing/ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3470cc47-5ed2-462c-b6ea-90bfaa96ffe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36f052c8-bd11-4d49-8962-f38c3bc42e03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "83d7e309-98d0-4cbc-ac58-7c21c8e88a05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "> Keep in mind that, we typically do not infer schema as the compute will be wasted to scan the data for the purpose of inferring the Schema. Instead we apply schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2691433-4efa-4283-a020-247dd24aa845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity_df. \\\n",
    "    writeStream. \\\n",
    "    format('memory'). \\\n",
    "    queryName('ghactivity'). \\\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM ghactivity').show()\n",
    "# We might not see the output as the data might not fit in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "06 Read JSON Data using Spark Structured Streaming",
   "notebookOrigID": 3881066461251976,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8cc93e2a-737a-44d5-99b0-348599f75916",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Analyze GHArchive Data in Parquet files using Spark\n",
    "\n",
    "Let us analyze GHArchive Data that is created by our Spark Structured Streaming Job.\n",
    "* Location: **/user/{username}/itv-github/streaming/bronze/data/ghactivity**.\n",
    "* As the files are in Parquet format, we can use `spark.read.parquet` to read these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Incremental Loads using Spark Structurd Streaming'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/${USER}/itv-github/streaming/bronze/data/ghactivity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef2da69d-01dd-43ee-a385-ff05ccc1bfd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity = spark. \\\n",
    "    read. \\\n",
    "    parquet(f\"/user/{username}/itv-github/streaming/bronze/data/ghactivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghactivity.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghactivity.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "742cb013-3647-4f3b-818b-849b5a3080f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity. \\\n",
    "  filter(\"type = 'CreateEvent' AND payload.ref_type = 'repository'\"). \\\n",
    "  count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a81cf7c-094b-4ff0-8a46-d74a0f4758e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also register Dataframe as temporary view and analyze the data using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f390ecf5-08b1-40fa-906a-3b82b2aaa06c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity.createOrReplaceTempView('ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc929cba-dc85-491e-b5ea-3eb7ff5c0490",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_repos = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    repo.id AS repo_id,\n",
    "    repo.name AS repo_name,\n",
    "    actor.id AS actor_id,\n",
    "    actor.login AS actor_login,\n",
    "    actor.display_login AS actor_display_login,\n",
    "    payload.ref_type AS ref_type,\n",
    "    type,\n",
    "    created_at,\n",
    "    year(created_at) AS year,\n",
    "    month(created_at) AS month,\n",
    "    dayofmonth(created_at) AS day\n",
    "  FROM ghactivity\n",
    "  WHERE type = 'CreateEvent'\n",
    "    AND payload.ref_type = 'repository'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "16bae9e0-a1db-4917-8cf7-6dca5ecb2148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9f23a84-5697-429d-8556-2718bebc26ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_repos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT count(1)\n",
    "  FROM ghactivity\n",
    "  WHERE type = 'CreateEvent'\n",
    "    AND payload.ref_type = 'repository'\n",
    "\"\"\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ca10388-68e9-4191-8971-fc194cda1493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Get count by date to confirm the counts by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f177ef52-7006-4e2e-977d-94f89837596d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11fd8bc7-baf0-4bc0-bde2-c716313a4fa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity. \\\n",
    "  groupby(to_date('created_at')). \\\n",
    "  count(). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e714c1c1-17b1-4ba1-b9ca-033ac7d2c7ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity. \\\n",
    "  select('type', 'payload.ref_type'). \\\n",
    "  distinct(). \\\n",
    "  show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "909c5693-e3df-4568-a10b-f2eea2b964d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity. \\\n",
    "  filter(\"payload.ref_type = 'repository'\"). \\\n",
    "  groupby('type'). \\\n",
    "  count(). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7854c57-823f-411a-970b-30f64d46d558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity. \\\n",
    "  filter(\"type = 'CreateEvent'\"). \\\n",
    "  groupby('payload.ref_type'). \\\n",
    "  count(). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e2d769aa-7fb9-441e-a7fa-04b4b3a369f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ghactivity.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "08 Analyze GHArchive Data in Delta files using Spark",
   "notebookOrigID": 3881066461251991,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
